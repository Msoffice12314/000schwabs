================================================================================
SCHWAB API MARKET DATA SYSTEM - COMPLETE RESEARCH AND IMPLEMENTATION GUIDE
================================================================================
Date: May 27, 2025
Purpose: Complete documentation for implementing Schwab API real-time streaming 
         and historical data collection system

================================================================================
SECTION 1: INITIAL RESEARCH FINDINGS - SCHWAB DEVELOPER API ANALYSIS
================================================================================

# Schwab Developer API: Comprehensive Analysis of OAuth Authentication and Market Data APIs

The Charles Schwab Developer API provides programmatic access to trading and market data through a robust OAuth 2.0 authentication framework. Based on comprehensive research of the authentication process, market data specifications, and API documentation, this report delivers a technical analysis of the current implementation as of May 2025.

## OAuth 2.0 Authentication Flow Requires Precise Implementation

The Schwab API implements the standard OAuth 2.0 authorization code flow with two critical endpoints:
- Authorization endpoint: https://api.schwabapi.com/v1/oauth/authorize
- Token endpoint: https://api.schwabapi.com/v1/oauth/token

The authentication process begins when users are redirected to Schwab's login microsite, where they authenticate with their trading credentials (not developer portal credentials) and authorize specific account access.

### Technical Implementation Requirements:
- **Authorization codes must be URL decoded before use** - a common pitfall that causes authentication failures
- Codes typically end with an '@' character after proper decoding
- Client credentials require Base64 encoding when constructing the Authorization header for token requests
- Exact format must be: Basic {base64_encoded_client_id:client_secret}
- Content-Type header must be set to: application/x-www-form-urlencoded

### Token Lifecycle:
- Access tokens expire after 30 minutes, requiring automatic refresh logic
- Refresh tokens expire after 7 days - a hard limit that cannot be extended
- This 7-day cycle represents a significant consideration for production deployments

## Market Data API Provides Comprehensive Real-time and Historical Data Access

The Market Data Production API operates from base URL: https://api.schwabapi.com/marketdata/v1

### Available Endpoints:
- /quotes - real-time pricing
- /pricehistory - historical OHLCV data
- /optionChains - options data with Greeks
- /marketHours - trading calendar information

### Key Specifications:
- **Rate limiting: 120 requests per minute** across all market data endpoints
- Approximately 100 symbols retrievable within each minute window
- Supports both REST requests for snapshot data and WebSocket connections for streaming
- Includes Level 2 order book data and time-and-sales information

### Authentication Requirements:
- Bearer token authorization required: Authorization: Bearer {access_token}
- Standard JSON responses with consistent formatting
- HTTP status codes: 401 for expired tokens, 429 for rate limit violations

## App Registration and Approval Process

### Configuration Requirements:
- Callback URL must be HTTPS-enabled and match exactly
- Most developers use https://127.0.0.1 for local development
- Recent reports indicate intermittent issues with localhost URLs

### Approval Timeline:
- **App approval typically takes 3-7 days** through manual review
- Applications begin in "Approved - Pending" status
- Must transition to "Ready for Use" before API access is granted
- No expedite process available

### Important Limitations:
- No official sandbox environment - all testing against production
- Requires careful implementation of dry-run capabilities
- Single application allowed per developer account

## Community Libraries and Support

Without official SDKs from Schwab, the developer community has created mature libraries:

### Python - schwab-py:
- Most comprehensive solution available
- Handles OAuth flows automatically
- Provides intuitive wrappers for all API endpoints
- Implements automatic token refresh and error handling

### R - schwabr:
- Available on CRAN
- Similar functionality to Python library
- Active community support

### Key Features of Community Libraries:
- Automatic token refresh with proper timing
- Exponential backoff for rate limits
- Secure credential management
- Solutions for self-signed certificate warnings during local OAuth flows
- Precise URL encoding requirements handling

## Technical Implementation Best Practices

### Security Considerations:
- **Encrypted storage mandatory for all credentials and tokens**
- Token files contain full account access
- Never share tokens between applications
- Implement comprehensive monitoring for authentication events

### Reliability Requirements:
- Proactive token refresh before 30-minute expiration
- Exponential backoff for rate limit violations
- Retry mechanisms for network timeouts
- Circuit breakers for API maintenance windows

================================================================================
SECTION 2: COMPLETE IMPLEMENTATION GUIDE
================================================================================

# Building a Robust Schwab Market Data System: Complete Implementation Guide

## 1. Realtime Streaming Data Implementation

The Schwab streaming API uses WebSocket connections to deliver real-time market data with sub-second latency.

### WebSocket Architecture Implementation:

```python
from schwab.streaming import StreamClient
import asyncio
import json

class SchwabStreamManager:
    def __init__(self, http_client, account_id):
        self.stream_client = StreamClient(http_client, account_id=account_id)
        self.active_subscriptions = set()
        self.reconnect_attempts = 0
        
    async def connect_and_stream(self):
        """Establish WebSocket connection with automatic reconnection"""
        while True:
            try:
                # Authenticate streaming session
                await self.stream_client.login()
                
                # Subscribe to market data streams
                await self.stream_client.level_one_equity_subs(
                    symbols=['AAPL', 'MSFT', 'GOOG'],
                    fields=[
                        StreamClient.LevelOneEquityFields.BID_PRICE,
                        StreamClient.LevelOneEquityFields.ASK_PRICE,
                        StreamClient.LevelOneEquityFields.LAST_PRICE,
                        StreamClient.LevelOneEquityFields.TOTAL_VOLUME
                    ]
                )
                
                # Subscribe to Level 2 order book data
                await self.stream_client.nasdaq_book_subs(['AAPL'])
                
                # Process incoming messages
                async for message in self.stream_client.handle_message():
                    await self.process_market_data(message)
                    
            except Exception as e:
                self.reconnect_attempts += 1
                wait_time = min(300, 2 ** self.reconnect_attempts)
                print(f"Connection lost, reconnecting in {wait_time}s...")
                await asyncio.sleep(wait_time)
```

### Supported Stream Types:
- Level One Quotes: Real-time bid/ask/last prices (once per second)
- Level Two Books: Full order book depth for NYSE and NASDAQ
- Chart Streams: OHLCV data for real-time charting
- Account Activity: Real-time position and order updates

## 2. Complete OAuth Implementation

```python
import requests
import base64
import json
from datetime import datetime, timedelta
from urllib.parse import urlencode

class SchwabAuthManager:
    def __init__(self, client_id, client_secret, redirect_uri):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.token_path = "schwab_tokens.json"
        self.auth_url = "https://api.schwabapi.com/v1/oauth/authorize"
        self.token_url = "https://api.schwabapi.com/v1/oauth/token"
        
    def get_authorization_url(self):
        """Generate OAuth authorization URL"""
        params = {
            'client_id': self.client_id,
            'redirect_uri': self.redirect_uri,
            'response_type': 'code'
        }
        return f"{self.auth_url}?{urlencode(params)}"
    
    def exchange_code_for_tokens(self, auth_code):
        """Exchange authorization code for access and refresh tokens"""
        # Create Basic Auth header
        credentials = base64.b64encode(
            f"{self.client_id}:{self.client_secret}".encode()
        ).decode()
        
        headers = {
            'Authorization': f'Basic {credentials}',
            'Content-Type': 'application/x-www-form-urlencoded'
        }
        
        data = {
            'grant_type': 'authorization_code',
            'code': auth_code,
            'redirect_uri': self.redirect_uri
        }
        
        response = requests.post(self.token_url, headers=headers, data=data)
        tokens = response.json()
        
        # Save tokens with expiration tracking
        tokens['access_token_expires'] = (
            datetime.now() + timedelta(seconds=tokens['expires_in'])
        ).isoformat()
        tokens['refresh_token_expires'] = (
            datetime.now() + timedelta(days=7)
        ).isoformat()
        
        self.save_tokens(tokens)
        return tokens
    
    def refresh_access_token(self):
        """Automatically refresh access token when expired"""
        tokens = self.load_tokens()
        
        # Check if refresh token is still valid
        refresh_expires = datetime.fromisoformat(tokens['refresh_token_expires'])
        if datetime.now() >= refresh_expires:
            raise Exception("Refresh token expired. Re-authentication required.")
        
        credentials = base64.b64encode(
            f"{self.client_id}:{self.client_secret}".encode()
        ).decode()
        
        headers = {
            'Authorization': f'Basic {credentials}',
            'Content-Type': 'application/x-www-form-urlencoded'
        }
        
        data = {
            'grant_type': 'refresh_token',
            'refresh_token': tokens['refresh_token']
        }
        
        response = requests.post(self.token_url, headers=headers, data=data)
        new_tokens = response.json()
        
        # Update token storage
        tokens.update(new_tokens)
        tokens['access_token_expires'] = (
            datetime.now() + timedelta(seconds=new_tokens['expires_in'])
        ).isoformat()
        
        self.save_tokens(tokens)
        return tokens
```

## 3. Historical Data Collection System

```python
class HistoricalDataCollector:
    def __init__(self, schwab_client):
        self.client = schwab_client
        self.base_url = "https://api.schwabapi.com/marketdata/v1"
        
    async def collect_price_history(self, symbol, period_type='year', 
                                  frequency_type='daily', frequency=1):
        """Retrieve historical OHLCV data"""
        params = {
            'periodType': period_type,
            'period': 1,
            'frequencyType': frequency_type,
            'frequency': frequency,
            'needExtendedHoursData': True
        }
        
        response = await self.client.get(
            f"{self.base_url}/pricehistory/{symbol}",
            params=params
        )
        
        return self.process_historical_data(response.json())
    
    def collect_options_history(self, symbol, expiration_date):
        """Collect historical options chain data"""
        params = {
            'symbol': symbol,
            'contractType': 'ALL',
            'includeQuotes': True,
            'strategy': 'SINGLE',
            'range': 'ALL',
            'expMonth': expiration_date.strftime('%b').upper(),
            'expYear': expiration_date.year
        }
        
        response = self.client.get(f"{self.base_url}/chains", params=params)
        return response.json()
    
    async def batch_collect_quotes(self, symbols):
        """Efficiently collect quotes for multiple symbols"""
        # API supports up to 100 symbols per request
        symbol_chunks = [symbols[i:i+100] for i in range(0, len(symbols), 100)]
        all_quotes = {}
        
        for chunk in symbol_chunks:
            params = {'symbols': ','.join(chunk)}
            response = await self.client.get(f"{self.base_url}/quotes", params=params)
            all_quotes.update(response.json())
            
        return all_quotes
```

### Data Collection Specifications:
- Minute data: Limited to 30-180 days of history
- Daily data: Multiple years available
- Batch processing: Up to 100 symbols per quote request
- Rate limiting: 120 requests per minute requires careful throttling

## 4. Production Architecture Design

```python
# Production architecture with microservices approach

class MarketDataPipeline:
    """Main orchestrator for market data collection and distribution"""
    
    def __init__(self):
        self.auth_manager = SchwabAuthManager()
        self.stream_manager = StreamManager()
        self.data_store = TimeSeriesDatabase()
        self.message_queue = MessageQueue()
        
    async def run(self):
        # Run components concurrently
        await asyncio.gather(
            self.stream_manager.run_streaming(),
            self.historical_collector.run_batch_collection(),
            self.data_processor.run_processing(),
            self.api_server.run()
        )

# Component separation for scalability
services = {
    'auth_service': {
        'responsibility': 'Token management and refresh',
        'scaling': 'Single instance with Redis token cache'
    },
    'stream_service': {
        'responsibility': 'WebSocket connection management',
        'scaling': 'Multiple instances for different asset classes'
    },
    'collector_service': {
        'responsibility': 'Historical data collection',
        'scaling': 'Horizontal scaling with rate limit coordination'
    },
    'storage_service': {
        'responsibility': 'Time-series data persistence',
        'scaling': 'Sharded by symbol and time range'
    }
}
```

### Infrastructure Components:
- Load Balancer: Distribute WebSocket connections across stream instances
- Message Queue: Kafka or Redis Streams for data distribution
- Time-Series Database: InfluxDB or TimescaleDB for market data
- Cache Layer: Redis for frequently accessed data
- Monitoring: Prometheus + Grafana for system observability

## 5. Comprehensive Error Management

```python
class ResilientMarketDataClient:
    def __init__(self):
        self.retry_config = {
            'max_attempts': 5,
            'base_delay': 1,
            'max_delay': 300,
            'exponential_base': 2
        }
        
    async def execute_with_retry(self, func, *args, **kwargs):
        """Execute API calls with exponential backoff retry"""
        attempt = 0
        last_exception = None
        
        while attempt < self.retry_config['max_attempts']:
            try:
                return await func(*args, **kwargs)
                
            except requests.HTTPError as e:
                if e.response.status_code == 429:  # Rate limited
                    retry_after = int(e.response.headers.get('Retry-After', 60))
                    await asyncio.sleep(retry_after)
                elif e.response.status_code == 401:  # Auth error
                    await self.auth_manager.refresh_access_token()
                elif e.response.status_code >= 500:  # Server error
                    delay = min(
                        self.retry_config['base_delay'] * 
                        (self.retry_config['exponential_base'] ** attempt),
                        self.retry_config['max_delay']
                    )
                    await asyncio.sleep(delay)
                else:
                    raise  # Don't retry client errors
                    
            except (ConnectionError, asyncio.TimeoutError) as e:
                last_exception = e
                delay = self.calculate_backoff_delay(attempt)
                await asyncio.sleep(delay)
                
            attempt += 1
        
        raise last_exception or Exception("Max retry attempts exceeded")
    
    def handle_stream_errors(self, error):
        """Specialized error handling for streaming connections"""
        error_handlers = {
            'AUTH_FAILED': self.handle_auth_failure,
            'CONNECTION_LOST': self.handle_connection_loss,
            'SUBSCRIPTION_FAILED': self.handle_subscription_failure,
            'HEARTBEAT_TIMEOUT': self.handle_heartbeat_timeout
        }
        
        handler = error_handlers.get(error.type, self.handle_generic_error)
        return handler(error)
```

### Critical Error Scenarios:
- Token expiration: Automatic refresh with 1-minute buffer
- Connection drops: Exponential backoff reconnection
- Rate limiting: Respect retry-after headers
- Service outages: Circuit breaker pattern implementation
- Data corruption: Validation and sanitization layers

## 6. Time-Series Database Schema

```sql
-- TimescaleDB schema optimized for market data
CREATE TABLE market_quotes (
    time TIMESTAMPTZ NOT NULL,
    symbol TEXT NOT NULL,
    bid DECIMAL(10,4),
    ask DECIMAL(10,4),
    last DECIMAL(10,4),
    volume BIGINT,
    bid_size INTEGER,
    ask_size INTEGER,
    PRIMARY KEY (symbol, time)
);

-- Create hypertable for automatic partitioning
SELECT create_hypertable('market_quotes', 'time', 
    chunk_time_interval => INTERVAL '1 day');

-- Create indexes for common queries
CREATE INDEX idx_symbol_time ON market_quotes (symbol, time DESC);
CREATE INDEX idx_time ON market_quotes (time DESC);

-- Compression policy for historical data
ALTER TABLE market_quotes SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'symbol'
);

SELECT add_compression_policy('market_quotes', 
    INTERVAL '7 days');
```

### Storage Architecture Patterns:
- Hot Storage: Recent 7 days in memory-optimized tables
- Warm Storage: 7-90 days in SSD-backed compressed format
- Cold Storage: >90 days in object storage (S3/GCS)
- Real-time Cache: Redis sorted sets for latest quotes
- Aggregations: Pre-computed OHLCV bars at multiple timeframes

## 7. WebSocket Connection Lifecycle Management

```python
class ConnectionManager:
    def __init__(self):
        self.connections = {}
        self.health_check_interval = 30
        self.market_calendar = MarketCalendar()
        
    async def maintain_connections(self):
        """Comprehensive connection management system"""
        while True:
            # Check market hours
            if not self.market_calendar.is_market_open():
                await self.disconnect_all()
                await self.wait_for_market_open()
                
            # Health check all connections
            for conn_id, connection in self.connections.items():
                if not await self.health_check(connection):
                    await self.reconnect(conn_id)
                    
            # Rebalance connections if needed
            await self.rebalance_connections()
            
            await asyncio.sleep(self.health_check_interval)
    
    async def health_check(self, connection):
        """Verify connection health with ping/pong"""
        try:
            pong = await connection.ping()
            return pong.received_at - pong.sent_at < 5.0
        except Exception:
            return False
    
    async def graceful_shutdown(self):
        """Clean shutdown procedure"""
        # Stop accepting new subscriptions
        self.accepting_new = False
        
        # Wait for in-flight messages
        await self.drain_message_queue()
        
        # Close connections gracefully
        for connection in self.connections.values():
            await connection.logout()
            await connection.close()
```

### Connection Best Practices:
- Single persistent connection per stream type
- Heartbeat monitoring every 30 seconds
- Market hours awareness for automatic start/stop
- Graceful reconnection with subscription state preservation
- Connection pooling not applicable (use single connection)

## 8. Rate Limit Management System

```python
class RateLimitManager:
    def __init__(self):
        self.limit = 120  # requests per minute
        self.window = 60  # seconds
        self.requests = deque()
        self.lock = asyncio.Lock()
        
    async def acquire(self):
        """Thread-safe rate limit enforcement"""
        async with self.lock:
            now = time.time()
            
            # Remove old requests outside window
            while self.requests and self.requests[0] < now - self.window:
                self.requests.popleft()
            
            # Check if limit reached
            if len(self.requests) >= self.limit:
                sleep_time = self.requests[0] + self.window - now
                await asyncio.sleep(sleep_time)
                return await self.acquire()
            
            # Record request
            self.requests.append(now)
            
    async def get_remaining_quota(self):
        """Get current rate limit status"""
        async with self.lock:
            now = time.time()
            active_requests = sum(1 for r in self.requests 
                                if r > now - self.window)
            return {
                'remaining': self.limit - active_requests,
                'reset_at': now + self.window,
                'limit': self.limit
            }
```

### Compliance Requirements:
- 120 requests/minute hard limit per application
- Data redistribution restrictions for market data
- Professional vs non-professional user designation
- Exchange agreements compliance for real-time data
- Audit trail requirements for financial applications

================================================================================
SECTION 3: MODULE ARCHITECTURE EXPLANATION
================================================================================

# Do You Need Different Modules for Streaming vs Historical Data?

Yes, you typically need two different modules for streaming real-time data and historical data, though they can share some common components.

## Why You Need Separate Modules

### 1. Different Data Sources and Protocols

```python
# Real-time streaming module uses WebSocket
class RealtimeStreamModule:
    def __init__(self):
        self.websocket_url = "wss://stream.schwabapi.com/ws"
        self.protocol = "WebSocket"
        self.data_format = "streaming JSON messages"
        
# Historical data module uses REST API
class HistoricalDataModule:
    def __init__(self):
        self.rest_url = "https://api.schwabapi.com/marketdata/v1"
        self.protocol = "HTTPS REST"
        self.data_format = "batch JSON responses"
```

### 2. Different Data Characteristics

| Aspect | Real-time Streaming | Historical Data |
|--------|-------------------|-----------------|
| Frequency | Sub-second updates | On-demand requests |
| Volume | Continuous small packets | Large batch downloads |
| Processing | Event-driven | Batch processing |
| Storage | In-memory buffer → Database | Direct to database |
| Error Recovery | Reconnect & continue | Retry failed requests |

### 3. Different Processing Requirements

```python
# Real-time module: Process individual ticks
class RealtimeProcessor:
    async def process_tick(self, tick_data):
        # Immediate processing for each update
        await self.update_current_price(tick_data)
        await self.check_trading_signals(tick_data)
        await self.update_live_charts(tick_data)
        
        # Buffer for batch insertion
        self.tick_buffer.append(tick_data)
        if len(self.tick_buffer) >= 100:
            await self.flush_to_database()

# Historical module: Process entire datasets
class HistoricalProcessor:
    async def process_historical_batch(self, symbol, timeframe):
        # Fetch complete dataset
        data = await self.fetch_price_history(symbol, timeframe)
        
        # Process all at once
        cleaned_data = self.clean_and_validate(data)
        aggregated_data = self.create_ohlcv_bars(cleaned_data)
        
        # Bulk insert
        await self.bulk_insert_to_database(aggregated_data)
```

## Recommended Architecture

### Core Module Structure

```python
# 1. Base module with shared functionality
class MarketDataBase:
    def __init__(self, auth_manager, db_connection):
        self.auth = auth_manager
        self.db = db_connection
        self.rate_limiter = RateLimiter()
        
    async def save_to_database(self, data, table_name):
        """Shared database saving logic"""
        pass
        
    def validate_market_data(self, data):
        """Common validation logic"""
        pass

# 2. Streaming module extending base
class StreamingModule(MarketDataBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.websocket_client = None
        self.subscription_manager = SubscriptionManager()
        self.tick_aggregator = TickAggregator()
        
    async def start_streaming(self, symbols):
        """WebSocket-specific implementation"""
        self.websocket_client = await self.connect_websocket()
        
        # Subscribe to real-time data
        await self.subscription_manager.subscribe(symbols)
        
        # Process incoming stream
        async for message in self.websocket_client:
            tick = self.parse_tick(message)
            
            # Real-time specific processing
            await self.process_realtime_tick(tick)
            
            # Aggregate for chart updates
            self.tick_aggregator.add_tick(tick)
            
    async def process_realtime_tick(self, tick):
        """Real-time specific logic"""
        # Update live charts
        await self.chart_updater.update_live(tick)
        
        # Check alerts
        await self.alert_manager.check(tick)
        
        # Buffer for efficient storage
        self.buffer.append(tick)
        if len(self.buffer) >= 100:
            await self.flush_buffer()

# 3. Historical module extending base
class HistoricalModule(MarketDataBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.http_client = HTTPClient()
        self.data_validator = HistoricalDataValidator()
        
    async def fetch_historical_data(self, symbol, period, interval):
        """REST API-specific implementation"""
        endpoint = f"/pricehistory/{symbol}"
        params = {
            'periodType': period,
            'frequencyType': interval,
            'needExtendedHoursData': True
        }
        
        # Rate-limited REST call
        async with self.rate_limiter:
            response = await self.http_client.get(endpoint, params)
            
        # Validate and process batch
        validated_data = self.data_validator.validate(response)
        
        # Transform to OHLCV format
        ohlcv_data = self.transform_to_ohlcv(validated_data)
        
        # Bulk save
        await self.bulk_save_historical(ohlcv_data)
        
    async def backfill_missing_data(self, symbol, start_date, end_date):
        """Historical-specific backfill logic"""
        missing_ranges = await self.find_data_gaps(symbol, start_date, end_date)
        
        for range_start, range_end in missing_ranges:
            data = await self.fetch_historical_data(
                symbol, 
                range_start, 
                range_end
            )
            await self.save_to_database(data, 'historical_prices')
```

## How They Work Together

### Unified Data Pipeline

```python
class UnifiedMarketDataSystem:
    def __init__(self):
        self.streaming = StreamingModule()
        self.historical = HistoricalModule()
        self.chart_generator = ChartGenerator()
        
    async def initialize_symbol(self, symbol):
        """Initialize both historical and real-time data"""
        # 1. Load historical data first
        await self.historical.fetch_historical_data(
            symbol, 
            period='1 year',
            interval='1 day'
        )
        
        # 2. Start streaming for real-time updates
        await self.streaming.start_streaming([symbol])
        
    async def generate_chart(self, symbol, timeframe):
        """Combine historical and real-time data for charts"""
        # Get historical data as base
        historical = await self.get_historical_from_db(
            symbol, 
            timeframe
        )
        
        # Get latest real-time data
        realtime = await self.streaming.get_recent_ticks(
            symbol,
            since=historical[-1]['timestamp']
        )
        
        # Merge and generate chart
        combined_data = self.merge_data_sources(historical, realtime)
        return self.chart_generator.create_chart(combined_data)
```

## Best Practices for Module Separation

### 1. Clear Interfaces

```python
# Define clear contracts between modules
from abc import ABC, abstractmethod

class IMarketDataProvider(ABC):
    @abstractmethod
    async def get_data(self, symbol, start, end):
        pass
        
    @abstractmethod
    async def subscribe(self, symbol, callback):
        pass
```

### 2. Shared Data Models

```python
from dataclasses import dataclass
from decimal import Decimal
from datetime import datetime

# Common data structures used by both modules
@dataclass
class MarketTick:
    symbol: str
    timestamp: datetime
    price: Decimal
    volume: int
    source: str  # 'realtime' or 'historical'
    
@dataclass
class OHLCV:
    timestamp: datetime
    open: Decimal
    high: Decimal
    low: Decimal
    close: Decimal
    volume: int
```

### 3. Event-Driven Communication

```python
class MarketDataEventBus:
    """Allow modules to communicate without tight coupling"""
    
    async def publish_new_data(self, event_type, data):
        # Streaming module publishes new ticks
        if event_type == 'new_tick':
            await self.chart_updater.handle_tick(data)
            await self.data_storage.buffer_tick(data)
            
        # Historical module publishes completed backfills
        elif event_type == 'backfill_complete':
            await self.chart_generator.regenerate_all()
```

## Module Comparison Summary

| Feature | Streaming Module | Historical Module | Shared Components |
|---------|-----------------|-------------------|-------------------|
| Protocol | WebSocket | REST API | Authentication |
| Data Flow | Push (continuous) | Pull (on-demand) | Data validation |
| Processing | Event-driven | Batch | Database models |
| Storage | Buffer → Database | Direct bulk insert | Storage layer |
| Error Handling | Reconnection | Retry with backoff | Logging |
| Performance | Low latency | High throughput | Rate limiting |

## Module Architecture Conclusion

You should maintain separate modules because:

1. **Different APIs**: WebSocket vs REST require different connection handling
2. **Different patterns**: Event-driven vs request-response
3. **Different optimizations**: Real-time needs low latency, historical needs throughput
4. **Easier maintenance**: Separate concerns make debugging and updates easier
5. **Better scalability**: Can scale streaming and historical processing independently

The modules should share common components (authentication, data models, storage interfaces) but maintain separate implementations for their specific data acquisition and processing needs.

================================================================================
IMPLEMENTATION SUMMARY AND KEY TAKEAWAYS
================================================================================

## Critical Implementation Points:

1. **Authentication Gotchas:**
   - Authorization codes MUST be URL decoded before use
   - Client credentials need Base64 encoding
   - 30-minute access tokens require proactive refresh
   - 7-day refresh token expiration is a hard limit

2. **API Limitations:**
   - 120 requests per minute rate limit
   - No official sandbox environment
   - 3-7 day manual app approval process
   - Single app per developer account

3. **Architecture Decisions:**
   - Use separate modules for streaming vs historical data
   - Implement robust error handling with exponential backoff
   - Use time-series databases for efficient storage
   - Plan for the 7-day re-authentication cycle

4. **Library Recommendations:**
   - Python: Use schwab-py for most mature implementation
   - R: Use schwabr package from CRAN
   - Both handle auth flows and rate limiting automatically

5. **Production Considerations:**
   - No 24/7 operation without weekly manual re-auth
   - WebSocket connections need health monitoring
   - Historical data limited to specific time ranges
   - Must handle market hours for connection management

## Next Steps for Implementation:

1. Start with schwab-py library installation
2. Register app on Schwab developer portal
3. Implement OAuth flow with proper error handling
4. Build separate modules for streaming and historical data
5. Set up TimescaleDB or similar for data storage
6. Implement monitoring and alerting
7. Test extensively before production deployment

================================================================================
END OF DOCUMENTATION
================================================================================